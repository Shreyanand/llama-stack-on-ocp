{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool call working examples with vLLM models without llamastack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_client import MCPClient\n",
    "import json, os\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "MCP_ENDPOINT = os.getenv(\"MCP_ENDPOINT\") \n",
    "LLS_ENDPOINT = os.getenv(\"REMOTE_BASE_URL\") \n",
    "LLS_OPENAI_ENDPOINT = f\"{LLS_ENDPOINT}/v1/openai/v1\"\n",
    "LLM_MODEL_ID = \"granite32-8b\"         \n",
    "OPENAI_APIKEY = os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=OPENAI_APIKEY, base_url=LLS_OPENAI_ENDPOINT)\n",
    "mcp = MCPClient(MCP_ENDPOINT)\n",
    "tools = await mcp.list_tools()                 \n",
    "openai_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": t.name,\n",
    "            \"description\": t.description,\n",
    "            \"parameters\": t.inputSchema, \n",
    "        },\n",
    "    }\n",
    "    for t in tools\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL\n",
      "generate_random_number\n",
      "{'min': '5', 'max': '50'}\n",
      "Results: {\"type\":\"text\",\"text\":\"49\",\"annotations\":null}\n",
      "\n",
      "ðŸ”¹ Assistant: I have used a random number generator to produce a number between 5 and 50. The result is 49.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Use tools to generate a number between 5 and 50\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "resp = await client.chat.completions.create(\n",
    "    model = LLM_MODEL_ID,\n",
    "    messages = messages,\n",
    "    tools = openai_tools,\n",
    "    tool_choice = \"auto\",\n",
    "    stream = False,\n",
    "    )\n",
    "assistant = resp.choices[0].message\n",
    "if assistant.tool_calls:\n",
    "    for call in assistant.tool_calls:\n",
    "        args = json.loads(call.function.arguments)\n",
    "        print(\"TOOL\")\n",
    "        print(call.function.name)\n",
    "        print(args)\n",
    "        result = await mcp.invoke_tool(call.function.name, args)\n",
    "        print(f\"Results: {result.content}\")\n",
    "\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"name\": call.function.name,\n",
    "                \"content\": result.content,  \n",
    "            }\n",
    "        )\n",
    "    final = await client.chat.completions.create(\n",
    "        model    = LLM_MODEL_ID,\n",
    "        messages = messages,\n",
    "        stream=False\n",
    "    )\n",
    "    print(\"\\nðŸ”¹ Assistant:\", final.choices[0].message.content)\n",
    "else:\n",
    "    print(\"\\nðŸ”¹ Assistant:\", assistant.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool call failed examples with vLLM models with llamastack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server @ http://llamasta... \n",
      "\n",
      "Your Server has access the the following toolgroups:\n",
      "{'builtin::code_interpreter', 'mcp::slack', 'mcp::custom_tool', 'builtin::websearch', 'mcp::openshift', 'builtin::rag'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "base_url = LLS_ENDPOINT\n",
    "mcp_url=MCP_ENDPOINT\n",
    "model = LLM_MODEL_ID\n",
    "\n",
    "client = LlamaStackClient(base_url=base_url)\n",
    "logger.info(f\"Connected to Llama Stack server @ {base_url[:15]}... \\n\")\n",
    "\n",
    "# Get tool info and register tools\n",
    "registered_tools = client.tools.list()\n",
    "registered_tools_identifiers = [t.identifier for t in registered_tools]\n",
    "registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "if \"mcp::custom_tool\" not in registered_toolgroups:\n",
    "    # Register MCP tools\n",
    "    client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::custom_tool\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":mcp_url},\n",
    "        )\n",
    "mcp_tools = [t.identifier for t in client.tools.list(toolgroup_id=\"mcp::custom_tool\")]\n",
    "\n",
    "logger.info(f\"\"\"Your Server has access the the following toolgroups:\n",
    "{set(registered_toolgroups)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m<\u001b[0m\u001b[33mtool\u001b[0m\u001b[33m_\u001b[0m\u001b[33mcall\u001b[0m\u001b[33m>\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create simple agent with tools\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=model,\n",
    "    instructions = \"\"\"\"\"\" ,\n",
    "    tools=[\"mcp::custom_tool\"],\n",
    "    tool_config={\"tool_choice\":\"auto\"},\n",
    "    sampling_params={\"max_tokens\": 4096}\n",
    ")\n",
    "\n",
    "\n",
    "user_prompts = [\"\"\"Use tools to generate a number between 5 and 50\"\"\"]\n",
    "session_id = agent.create_session(session_name=\"Auto_demo\")\n",
    "for prompt in user_prompts:\n",
    "    turn_response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    for log in EventLogger().log(turn_response):\n",
    "        log.print()\n",
    "\n",
    "logger.handlers.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
