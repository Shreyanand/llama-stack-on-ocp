{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool call with vLLM models with llamastack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_client import MCPClient\n",
    "import json, os\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "LLS_ENDPOINT = os.getenv(\"REMOTE_BASE_URL\") \n",
    "LLS_OPENAI_ENDPOINT = f\"{LLS_ENDPOINT}/v1/openai/v1\"\n",
    "OPENAI_APIKEY = os.getenv(\"OPENAI_API_KEY\", \"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchtune(query: str = \"torchtune\"):\n",
    "    \"\"\"\n",
    "    Answer information about torchtune.\n",
    "\n",
    "    :param query: The query to use for querying the internet\n",
    "    :returns: Information about torchtune\n",
    "    \"\"\"\n",
    "    dummy_response = \"\"\"\n",
    "    torchtune is a PyTorch library for easily authoring, finetuning and experimenting with LLMs.\n",
    "\n",
    "    torchtune provides:\n",
    "\n",
    "    PyTorch implementations of popular LLMs from Llama, Gemma, Mistral, Phi, and Qwen model families\n",
    "    Hackable training recipes for full finetuning, LoRA, QLoRA, DPO, PPO, QAT, knowledge distillation, and more\n",
    "    Out-of-the-box memory efficiency, performance improvements, and scaling with the latest PyTorch APIs\n",
    "    YAML configs for easily configuring training, evaluation, quantization or inference recipes\n",
    "    Built-in support for many popular dataset formats and prompt templates\n",
    "    \"\"\"\n",
    "    return dummy_response\n",
    "\n",
    "def test_lls(mcp_endpoint, mcp_toolgroup, model, instructions, prompts):\n",
    "    client = LlamaStackClient(base_url=LLS_ENDPOINT)\n",
    "    logger.info(f\"Connected to Llama Stack server @ {LLS_ENDPOINT[:15]}... \\n\")\n",
    "\n",
    "    # Get tool info and register tools\n",
    "    registered_tools = client.tools.list()\n",
    "    registered_tools_identifiers = [t.identifier for t in registered_tools]\n",
    "    registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "    if mcp_toolgroup not in registered_toolgroups:\n",
    "        # Register MCP tools\n",
    "        client.toolgroups.register(\n",
    "            toolgroup_id=mcp_toolgroup,\n",
    "            provider_id=\"model-context-protocol\",\n",
    "            mcp_endpoint={\"uri\":mcp_endpoint},\n",
    "            )\n",
    "    mcp_tools = [t.identifier for t in client.tools.list(toolgroup_id=mcp_toolgroup)]\n",
    "\n",
    "    logger.info(f\"\"\"Your Server has access the the following toolgroups:\n",
    "    {set(registered_toolgroups)}\n",
    "    \"\"\")\n",
    "    # Create simple agent with tools\n",
    "    agent = Agent(\n",
    "        client,\n",
    "        model=model,\n",
    "        instructions = instructions,\n",
    "        tools=[mcp_toolgroup, torchtune],\n",
    "        tool_config={\"tool_choice\":\"auto\"},\n",
    "        sampling_params={\"max_tokens\": 4096}\n",
    "    )\n",
    "\n",
    "\n",
    "    user_prompts = prompts\n",
    "    session_id = agent.create_session(session_name=\"Auto_demo\")\n",
    "    for prompt in user_prompts:\n",
    "        turn_response = agent.create_turn(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            session_id=session_id,\n",
    "            stream=True,\n",
    "        )\n",
    "        for log in EventLogger().log(turn_response):\n",
    "            if \"Tool:pods_list_in_namespace Response:\" in log.content:\n",
    "                continue\n",
    "            log.print()\n",
    "\n",
    "    logger.handlers.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[33m<\u001b[0m\u001b[33mtool\u001b[0m\u001b[33m_\u001b[0m\u001b[33mcall\u001b[0m\u001b[33m>\u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "MCP_TOOLGROUP= \"mcp::custom_tool\"\n",
    "MCP_ENDPOINT= \"\"\n",
    "LLM_MODEL_ID = \"granite32-8b\"\n",
    "INSTRUCTIONS = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\"\"\"      \n",
    "PROMPT = \"Use tools to generate a number between 5 and 50\"\n",
    "test_lls(MCP_ENDPOINT, MCP_TOOLGROUP, LLM_MODEL_ID, INSTRUCTIONS, [PROMPT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[32mtool_execution> Tool:pods_list_in_namespace Args:{'namespace': 'llama-serve'}\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[30m\u001b[0m"
     ]
    }
   ],
   "source": [
    "MCP_TOOLGROUP= \"mcp::openshift\"\n",
    "MCP_ENDPOINT=\"\"\n",
    "LLM_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "INSTRUCTIONS = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\"\"\"      \n",
    "PROMPT = \"List the pods in llama serve namespace\"\n",
    "test_lls(MCP_ENDPOINT, MCP_TOOLGROUP, LLM_MODEL_ID, INSTRUCTIONS, [PROMPT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the infamous out of token error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minference> \u001b[0m\u001b[97m\u001b[0m\n",
      "\u001b[31m500: Internal server error: An unexpected error occurred.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MCP_TOOLGROUP= \"mcp::github\"\n",
    "MCP_ENDPOINT=os.getenv(\"GITHUB_MCP_SERVER_URL\") \n",
    "LLM_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "INSTRUCTIONS = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone. \n",
    "For parameters like 'page', 'perPage', 'limit', etc., please ensure you provide numeric values without quotes (example: \"page\": 1 not \"page\": \"1\").\"\"\"         \n",
    "PROMPT = \"Search for top 5 Python repositories related to 'llama', sorted by stars.\"\n",
    "test_lls(MCP_ENDPOINT, MCP_TOOLGROUP, LLM_MODEL_ID, INSTRUCTIONS, [PROMPT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool call with vLLM models without llamastack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_openai_api(mcp_endpoint, model, instruction, prompt):\n",
    "    client = AsyncOpenAI(api_key=OPENAI_APIKEY, base_url=LLS_OPENAI_ENDPOINT)\n",
    "    mcp = MCPClient(mcp_endpoint)\n",
    "    tools = await mcp.list_tools()                 \n",
    "    openai_tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": t.name,\n",
    "                \"description\": t.description,\n",
    "                \"parameters\": t.inputSchema, \n",
    "            },\n",
    "        }\n",
    "        for t in tools\n",
    "    ]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (instruction)\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (prompt),\n",
    "        }\n",
    "    ]\n",
    "    resp = await client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        tools = openai_tools,\n",
    "        tool_choice = \"auto\",\n",
    "        stream = False,\n",
    "        )\n",
    "    assistant = resp.choices[0].message\n",
    "    if assistant.tool_calls:\n",
    "        for call in assistant.tool_calls:\n",
    "            args = json.loads(call.function.arguments)\n",
    "            print(\"TOOL\")\n",
    "            print(call.function.name)\n",
    "            print(args)\n",
    "            result = await mcp.invoke_tool(call.function.name, args)\n",
    "            #print(f\"Results: {result.content}\")\n",
    "\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"name\": call.function.name,\n",
    "                    \"content\": result.content,  \n",
    "                }\n",
    "            )\n",
    "        final = await client.chat.completions.create(\n",
    "            model    = model,\n",
    "            messages = messages,\n",
    "            stream=False\n",
    "        )\n",
    "        print(\"\\n🔹 Assistant:\", final.choices[0].message.content)\n",
    "    else:\n",
    "        print(\"\\n🔹 Assistant:\", assistant.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL\n",
      "generate_random_number\n",
      "{'min': '5', 'max': '50'}\n",
      "Results: {\"type\":\"text\",\"text\":\"23\",\"annotations\":null}\n",
      "\n",
      "🔹 Assistant: I've generated a number for you! It's 23.\n"
     ]
    }
   ],
   "source": [
    "MCP_ENDPOINT = os.getenv(\"MCP_ENDPOINT\") \n",
    "LLM_MODEL_ID = \"granite32-8b\"\n",
    "INSTRUCTIONS = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\"\"\"      \n",
    "PROMPT = \"Use tools to generate a number between 5 and 50\"\n",
    "await test_openai_api(MCP_ENDPOINT, LLM_MODEL_ID, INSTRUCTIONS, PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL\n",
      "pods_list_in_namespace\n",
      "{'namespace': 'llama-serve'}\n",
      "\n",
      "🔹 Assistant: Here is the list of pods in the `llama-serve` namespace:\n",
      "\n",
      "1. `ansible-mcp-server-6d8d74d699-9bb6l`\n",
      "2. `auto-quote-mcp-5d579cbbfb-4vstt`\n",
      "3. `custom-mcp-server-59cdf5cfd7-6 bwSchool]\n",
      "4. `graveeteurne-rich25- enthusiastic-santonia`\n",
      "5. `github-mcp-server-with-rh-nodejs-7b4dd84f68-qj7qk`\n",
      "6. `granite-8b`\n",
      "7. `granite32-8b-predictor-00001-deployment-7dc6885d57-rbd4t`\n",
      "8. `granite33-8b`\n",
      "9. `llama32-3b`\n",
      "10. `llamastack-deployment-956c577f4-cs9ls`\n",
      "11. `mcp-llamastack-server-667fc898c-n57mp`\n",
      "12. `ocp-mcp-server-7cbd674668-sjrmj`\n",
      "13. `simple-mcp-server-677b86fb84-skfk6`\n",
      "14. `slack-mcp-server-76cdf9bc7b-h5456`\n",
      "15. `slack-test`\n",
      "16. `streamlit-79649d549d-h5456`\n"
     ]
    }
   ],
   "source": [
    "MCP_ENDPOINT = os.getenv(\"MCP_ENDPOINT_OCP\") \n",
    "LLM_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "INSTRUCTIONS = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone. \"\"\"\n",
    "PROMPT = \"List the pods in llama serve namespace\"\n",
    "await test_openai_api(MCP_ENDPOINT, LLM_MODEL_ID, INSTRUCTIONS, PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL\n",
      "search_repositories\n",
      "{'query': 'python llama', 'page': '1', 'perPage': '5'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/shrey/llama-stack-on-ocp/tests/mcp_client.py\", line 124, in invoke_tool\n",
      "  |     async with ClientSession(*streams) as session:\n",
      "  |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 767, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/shrey/llama-stack-on-ocp/tests/mcp_client.py\", line 126, in invoke_tool\n",
      "    |     result = await session.call_tool(tool_name, kwargs)\n",
      "    |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/client/session.py\", line 225, in call_tool\n",
      "    |     return await self.send_request(\n",
      "    |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/shared/session.py\", line 250, in send_request\n",
      "    |     raise McpError(response_or_error.error)\n",
      "    | mcp.shared.exceptions.McpError: Invalid input: [{\"code\":\"invalid_type\",\"expected\":\"number\",\"received\":\"string\",\"path\":[\"page\"],\"message\":\"Expected number, received string\"},{\"code\":\"invalid_type\",\"expected\":\"number\",\"received\":\"string\",\"path\":[\"perPage\"],\"message\":\"Expected number, received string\"}]\n",
      "    +------------------------------------\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/6z/yvfzhhxd3hlck120_g5plslr0000gn/T/ipykernel_51266/3266449598.py\", line 7, in <module>\n",
      "  |     await test_openai_api(MCP_ENDPOINT, LLM_MODEL_ID, INSTRUCTIONS, PROMPT)\n",
      "  |   File \"/var/folders/6z/yvfzhhxd3hlck120_g5plslr0000gn/T/ipykernel_51266/1757216760.py\", line 40, in test_openai_api\n",
      "  |     result = await mcp.invoke_tool(call.function.name, args)\n",
      "  |   File \"/Users/shrey/llama-stack-on-ocp/tests/mcp_client.py\", line 123, in invoke_tool\n",
      "  |     async with sse_client(self.endpoint) as streams:\n",
      "  |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/contextlib.py\", line 217, in __aexit__\n",
      "  |     await self.gen.athrow(typ, value, traceback)\n",
      "  |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/client/sse.py\", line 43, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 767, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/shrey/llama-stack-on-ocp/tests/mcp_client.py\", line 126, in invoke_tool\n",
      "    |     result = await session.call_tool(tool_name, kwargs)\n",
      "    |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/client/session.py\", line 225, in call_tool\n",
      "    |     return await self.send_request(\n",
      "    |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/shared/session.py\", line 250, in send_request\n",
      "    |     raise McpError(response_or_error.error)\n",
      "    | mcp.shared.exceptions.McpError: Invalid input: [{\"code\":\"invalid_type\",\"expected\":\"number\",\"received\":\"string\",\"path\":[\"page\"],\"message\":\"Expected number, received string\"},{\"code\":\"invalid_type\",\"expected\":\"number\",\"received\":\"string\",\"path\":[\"perPage\"],\"message\":\"Expected number, received string\"}]\n",
      "    | \n",
      "    | During handling of the above exception, another exception occurred:\n",
      "    | \n",
      "    | Exception Group Traceback (most recent call last):\n",
      "    |   File \"/Users/shrey/llama-stack-on-ocp/tests/mcp_client.py\", line 124, in invoke_tool\n",
      "    |     async with ClientSession(*streams) as session:\n",
      "    |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 767, in __aexit__\n",
      "    |     raise BaseExceptionGroup(\n",
      "    | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "    +-+---------------- 1 ----------------\n",
      "      | Traceback (most recent call last):\n",
      "      |   File \"/Users/shrey/llama-stack-on-ocp/tests/mcp_client.py\", line 126, in invoke_tool\n",
      "      |     result = await session.call_tool(tool_name, kwargs)\n",
      "      |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/client/session.py\", line 225, in call_tool\n",
      "      |     return await self.send_request(\n",
      "      |   File \"/Users/shrey/miniforge3/envs/stack/lib/python3.10/site-packages/mcp/shared/session.py\", line 250, in send_request\n",
      "      |     raise McpError(response_or_error.error)\n",
      "      | mcp.shared.exceptions.McpError: Invalid input: [{\"code\":\"invalid_type\",\"expected\":\"number\",\"received\":\"string\",\"path\":[\"page\"],\"message\":\"Expected number, received string\"},{\"code\":\"invalid_type\",\"expected\":\"number\",\"received\":\"string\",\"path\":[\"perPage\"],\"message\":\"Expected number, received string\"}]\n",
      "      +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "MCP_ENDPOINT = os.getenv(\"MCP_ENDPOINT_GITHUB\") \n",
    "LLM_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "INSTRUCTIONS = \"\"\"You are a helpful assistant. You have access to a number of tools.\n",
    "Whenever a tool is called, be sure return the Response in a friendly and helpful tone. \n",
    "For parameters like 'page', 'perPage', 'limit', etc., please ensure you provide numeric values without quotes (example: \"page\": 1 not \"page\": \"1\").\"\"\"         \n",
    "PROMPT = \"Search for top 5 Python repositories related to 'llama', sorted by stars.\"\n",
    "await test_openai_api(MCP_ENDPOINT, LLM_MODEL_ID, INSTRUCTIONS, PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug openapi\n",
    "# client = AsyncOpenAI(api_key=OPENAI_APIKEY, base_url=LLS_OPENAI_ENDPOINT)\n",
    "# mcp = MCPClient(MCP_ENDPOINT)\n",
    "# tools = await mcp.list_tools()                 \n",
    "# openai_tools = [\n",
    "#     {\n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": t.name,\n",
    "#             \"description\": t.description,\n",
    "#             \"parameters\": t.inputSchema, \n",
    "#         },\n",
    "#     }\n",
    "#     for t in tools\n",
    "# ]\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": (\n",
    "#             \"Use tools to generate a number between 5 and 50\"\n",
    "#         ),\n",
    "#     }\n",
    "# ]\n",
    "# resp = await client.chat.completions.create(\n",
    "#     model = LLM_MODEL_ID,\n",
    "#     messages = messages,\n",
    "#     tools = openai_tools,\n",
    "#     tool_choice = \"auto\",\n",
    "#     stream = False,\n",
    "#     )\n",
    "# assistant = resp.choices[0].message\n",
    "# if assistant.tool_calls:\n",
    "#     for call in assistant.tool_calls:\n",
    "#         args = json.loads(call.function.arguments)\n",
    "#         print(\"TOOL\")\n",
    "#         print(call.function.name)\n",
    "#         print(args)\n",
    "#         result = await mcp.invoke_tool(call.function.name, args)\n",
    "#         print(f\"Results: {result.content}\")\n",
    "\n",
    "#         messages.append(\n",
    "#             {\n",
    "#                 \"role\": \"assistant\",\n",
    "#                 \"name\": call.function.name,\n",
    "#                 \"content\": result.content,  \n",
    "#             }\n",
    "#         )\n",
    "#     final = await client.chat.completions.create(\n",
    "#         model    = LLM_MODEL_ID,\n",
    "#         messages = messages,\n",
    "#         stream=False\n",
    "#     )\n",
    "#     print(\"\\n🔹 Assistant:\", final.choices[0].message.content)\n",
    "# else:\n",
    "#     print(\"\\n🔹 Assistant:\", assistant.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug lls\n",
    "# client = LlamaStackClient(base_url=LLS_ENDPOINT)\n",
    "# logger.info(f\"Connected to Llama Stack server @ {LLS_ENDPOINT[:15]}... \\n\")\n",
    "\n",
    "# # Get tool info and register tools\n",
    "# registered_tools = client.tools.list()\n",
    "# registered_tools_identifiers = [t.identifier for t in registered_tools]\n",
    "# registered_toolgroups = [t.toolgroup_id for t in registered_tools]\n",
    "\n",
    "# if mcp_toolgroup not in registered_toolgroups:\n",
    "#     # Register MCP tools\n",
    "#     client.toolgroups.register(\n",
    "#         toolgroup_id=mcp_toolgroup,\n",
    "#         provider_id=\"model-context-protocol\",\n",
    "#         mcp_endpoint={\"uri\":mcp_endpoint},\n",
    "#         )\n",
    "# mcp_tools = [t.identifier for t in client.tools.list(toolgroup_id=mcp_toolgroup)]\n",
    "\n",
    "# logger.info(f\"\"\"Your Server has access the the following toolgroups:\n",
    "# {set(registered_toolgroups)}\n",
    "# \"\"\")\n",
    "# # Create simple agent with tools\n",
    "# agent = Agent(\n",
    "#     client,\n",
    "#     model=model,\n",
    "#     instructions = \"\"\"\"\"\" ,\n",
    "#     tools=[\"mcp::custom_tool\"],\n",
    "#     tool_config={\"tool_choice\":\"auto\"},\n",
    "#     sampling_params={\"max_tokens\": 4096}\n",
    "# )\n",
    "\n",
    "\n",
    "# user_prompts = [\"\"\"Use tools to generate a number between 5 and 50\"\"\"]\n",
    "# session_id = agent.create_session(session_name=\"Auto_demo\")\n",
    "# for prompt in user_prompts:\n",
    "#     turn_response = agent.create_turn(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\":\"user\",\n",
    "#                 \"content\": prompt\n",
    "#             }\n",
    "#         ],\n",
    "#         session_id=session_id,\n",
    "#         stream=True,\n",
    "#     )\n",
    "#     for log in EventLogger().log(turn_response):\n",
    "#         log.print()\n",
    "\n",
    "# logger.handlers.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
